{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on generating an explanation for an image-based model on Watson OpenScale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes steps for creating an image-based watson-machine-learning model, creating a subscription, configuring explainability, and finally generating an explanation for a transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "- [1. Setup](#setup)\n",
    "- [2. Creating and deploying an image-based model](#deployment)\n",
    "- [3. Subscriptions](#subscription)\n",
    "- [4. Explainability](#explainability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If using Watson Studio, try running the notebook on at least 'Default Python 3.5 S' version for faster results (vs Python XS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Setup\n",
    "\n",
    "### 1.1 Install Watson OpenScale and WML packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ibm-ai-openscale\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/f9/5167f4954c06351f7e65365c9af475edfab96d8f424e2c772d4c1c3c9802/ibm_ai_openscale-2.1.17-py3-none-any.whl (537kB)\n",
      "\u001b[K     |████████████████████████████████| 542kB 15.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pandas in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-ai-openscale) (0.24.1)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-ai-openscale) (0.8.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-ai-openscale) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-ai-openscale) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas->ibm-ai-openscale) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas->ibm-ai-openscale) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas->ibm-ai-openscale) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->ibm-ai-openscale) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->ibm-ai-openscale) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->ibm-ai-openscale) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests->ibm-ai-openscale) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/Python36/lib/python3.6/site-packages (from h5py->ibm-ai-openscale) (1.12.0)\n",
      "Installing collected packages: ibm-ai-openscale\n",
      "Successfully installed ibm-ai-openscale-2.1.17\n",
      "Successfully installed watson-machine-learning-client-1.0.376\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade ibm-ai-openscale\n",
    "!pip install --upgrade watson-machine-learning-client --no-cache | tail -n 1\n",
    "\n",
    "# !pip install --upgrade watson-machine-learning-client --no-cache | tail -n 1\n",
    "# !pip install watson-machine-learning-client==1.0.371\n",
    "\n",
    "#  !pip install watson-machine-learning-client==1.0.375\n",
    "# !pip install --upgrade ibm-ai-openscale\n",
    "\n",
    "# !pip install --upgrade ibm-ai-openscale --no-cache | tail -n 1\n",
    "# !pip install ibm-ai-openscale==2.1.16\n",
    "\n",
    "# !pip install --upgrade watson-machine-learning-client --no-cache | tail -n 1\n",
    "# !pip install watson-machine-learning-client==1.0.371"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Restart the kernel to assure the new libraries are being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Configure credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this Lab you need to have a valid instance of Watson Openscale.\n",
    "\n",
    "To verify if you have one, go to the [cloud console](https://cloud.ibm.com/resources), clicking on `Services` you should see your Watson OpenScale instance listed.\n",
    "\n",
    "if not then from that screen click the upper right button **\"Create ressource\"**.\n",
    "From the search entry type 'openscale' and create a lite plan of Watson OpenScale.\n",
    "\n",
    "You also need a valid **IBM Cloud API Key** to assign the variable in the next cell.\n",
    "\n",
    "To get it go to the [IBM Cloud console](https://console.bluemix.net/) then click from the upper toolbar `Manage->Access (IAM)`. \n",
    "Select `IBM Cloud API Keys` from the left hand sidebar and then click the **\"Create an IBM Cloud API Key\"** button.\n",
    "\n",
    "From that page, give your key a name and click Create, then copy the created key and paste it below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOUD_API_KEY = \"<insert your own CLOUD-API-KEY here>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70ee9046-f34e-441c-8dbe-75d57d88b6f7\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from ibm_ai_openscale.utils import get_instance_guid\n",
    "\n",
    "WOS_GUID = get_instance_guid(api_key=CLOUD_API_KEY)\n",
    "AIOS_CREDENTIALS = {\n",
    "    \"instance_guid\": WOS_GUID,\n",
    "    \"apikey\": CLOUD_API_KEY,\n",
    "    \"url\": \"https://api.aiopenscale.cloud.ibm.com\"\n",
    "}\n",
    "\n",
    "if WOS_GUID is None:\n",
    "    print('Watson OpenScale GUID NOT FOUND')\n",
    "else:\n",
    "    print(WOS_GUID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also need to have a valid instance of Watson Machine Learning (runtime for your models) running.\n",
    "\n",
    "To verify if you have one, go to the [cloud console](https://cloud.ibm.com/resources), clicking on `Services` you should see your Watson Machine Learning instance listed.\n",
    "\n",
    "if not then from that screen click the upper right button **\"Create ressource\"**.\n",
    "From the search entry type 'Machine Learning' and create a lite plan of Watson Machine Learning. \n",
    "MAKE SURE THE REGION FIELD GOT **DALLAS** as value if not modify it accordingly.\n",
    "\n",
    "From the IBM Cloud Resource list click on the Watson Machine Learning instance and from this page click the service credentials side bar item.\n",
    "clik on view **'credentials'** and copy the all json info provided as follow :\n",
    "\n",
    "```\n",
    "{\n",
    "  \"apikey\": \"XXXXXXXXXX\",\n",
    "  \"iam_apikey_description\": \"Auto-generated for key XXXX-YYYYY-ZZZZZZ\",\n",
    "  \"iam_apikey_name\": \"WML-credentials\",\n",
    "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n",
    "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/XXXXXXXX::serviceid:ServiceId-XXXX-YYYYYY-ZZZZZZZZ\",\n",
    "  \"instance_id\": \"WWWWWWWWWWWWWWWW\",\n",
    "  \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
    "}\n",
    "```   \n",
    "    \n",
    "\n",
    "replace the following variable with the obtained json data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WML_CREDENTIALS = {\n",
    "  \"apikey\": \"xxxxxxxxxxxxxx\",\n",
    "  \"iam_apikey_description\": \"Auto-generated for key yyyyyyyyyyyyy\",\n",
    "  \"iam_apikey_name\": \"Service credentials-WML4JLC\",\n",
    "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",zzzzz\",\n",
    "  \"instance_id\": \"xxxxxxx\",\n",
    "  \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "import sys, time\n",
    "\n",
    "def Wait(seconds, Speed=5):\n",
    "    Chars = [\"|\",\"/\",\"-\",\"\\\\\"]\n",
    "    MaxChars = 4\n",
    "    sys.stdout.flush()\n",
    "    for i in range(seconds*Speed):\n",
    "        sys.stdout.write(\"\\r\" + Chars[i % MaxChars])\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(1/Speed)\n",
    "    sys.stdout.write(\"\\r \")\n",
    "Wait(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deployment\"></a>\n",
    "## 2. Creating and deploying an image-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used is MNIST dataset of handwritten digits. It consists of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. More information about the dataset can be found here: https://keras.io/datasets/#mnist-database-of-handwritten-digits\n",
    "\n",
    "Note: Tensorflow versions supported by WML are: 1.2, 1.5, and 1.11. Make sure you have one of these versions before creating the models. Version 1.11 is used in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/conda/envs/Python36/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras) (1.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras) (3.13)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: keras_applications>=1.0.6 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras) (1.0.6)\n",
      "Requirement already satisfied: keras_preprocessing>=1.0.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras) (1.0.5)\n",
      "Collecting tensorflow==1.11.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/d5/38cd4543401708e64c9ee6afa664b936860f4630dd93a49ab863f9998cd2/tensorflow-1.11.0-cp36-cp36m-manylinux1_x86_64.whl (63.0MB)\n",
      "\u001b[K     |████████████████████████████████| 63.0MB 45.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow==1.11.0) (1.0.6)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow==1.11.0) (1.12.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow==1.11.0) (0.32.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow==1.11.0) (1.0.5)\n",
      "Collecting tensorboard<1.12.0,>=1.11.0 (from tensorflow==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/2f/4d788919b1feef04624d63ed6ea45a49d1d1c834199ec50716edb5d310f4/tensorboard-1.11.0-py3-none-any.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 39.7MB/s eta 0:00:01:01�█████████████▏ | 2.9MB 39.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow==1.11.0) (0.7.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow==1.11.0) (3.6.1)\n",
      "Collecting setuptools<=39.1.0 (from tensorflow==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl (566kB)\n",
      "\u001b[K     |████████████████████████████████| 573kB 37.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow==1.11.0) (1.16.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow==1.11.0) (0.7.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow==1.11.0) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow==1.11.0) (0.2.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorflow==1.11.0) (1.15.4)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras-applications>=1.0.5->tensorflow==1.11.0) (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0) (3.0.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0) (0.14.1)\n",
      "Installing collected packages: tensorboard, setuptools, tensorflow\n",
      "  Found existing installation: setuptools 40.8.0\n",
      "    Uninstalling setuptools-40.8.0:\n",
      "      Successfully uninstalled setuptools-40.8.0\n",
      "  Found existing installation: tensorflow 1.13.1\n",
      "    Uninstalling tensorflow-1.13.1:\n",
      "      Successfully uninstalled tensorflow-1.13.1\n",
      "Successfully installed setuptools-39.1.0 tensorboard-1.11.0 tensorflow-1.11.0\n",
      "Collecting keras_sequential_ascii\n",
      "  Downloading https://files.pythonhosted.org/packages/2d/a4/806e3ed5d7ac7463e2fae77e09ccccc88c78266b248fb637e4efa4f65ec0/keras_sequential_ascii-0.1.1.tar.gz\n",
      "Requirement already satisfied: keras in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras_sequential_ascii) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras->keras_sequential_ascii) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras->keras_sequential_ascii) (1.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras->keras_sequential_ascii) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras->keras_sequential_ascii) (3.13)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras->keras_sequential_ascii) (2.9.0)\n",
      "Requirement already satisfied: keras_applications>=1.0.6 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras->keras_sequential_ascii) (1.0.6)\n",
      "Requirement already satisfied: keras_preprocessing>=1.0.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from keras->keras_sequential_ascii) (1.0.5)\n",
      "Building wheels for collected packages: keras-sequential-ascii\n",
      "  Building wheel for keras-sequential-ascii (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/f5/8d/81/912666dff82a923ce423a7e797cd75f54271c7031512cdb282\n",
      "Successfully built keras-sequential-ascii\n",
      "Installing collected packages: keras-sequential-ascii\n",
      "Successfully installed keras-sequential-ascii-0.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow==1.11.0\n",
    "!pip install keras_sequential_ascii\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras_sequential_ascii import sequential_model_to_ascii_printout\n",
    "from keras import backend as keras_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KERAS v 2.2.4\n",
      "TENSORFLOW v 1.11.0\n"
     ]
    }
   ],
   "source": [
    "print(\"KERAS v {}\".format(keras.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TENSORFLOW v {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if keras_backend.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you don't want to train the model during this lab, which is quite time consumming (15 mn average) depending on the size of your python/jupyter environment, you can use a pre-trained model provided to you with this notebook file.\n",
    "You also have the definition and trained weights of the model in a file called **HandWrittenDigit-CNN.h5**\n",
    "\n",
    "**Keras also supports a simpler interface to save both the model weights and model architecture together into a single H5 file, \n",
    "while the HDF5 format store only Model weights and therefore the model architecture is provided as a JSON format.**\n",
    "    \n",
    "* Saving/Loading the model in H5 includes everything we need to know about the model, including:\n",
    "    - Model weights.\n",
    "    - Model architecture.\n",
    "    - Model compilation details (loss and metrics).\n",
    "    - Model optimizer state.\n",
    "* This means that we can load and use the model directly, without having to re-compile it.\n",
    "\n",
    "\n",
    "To upload the HD5 file and use it please procedd as follow :\n",
    "\n",
    "From the upper toolbar select the *01* icon and Files tab, then drag/drop the file **HandWrittenDigit-CNN.h5** provided in the box folder\n",
    "\n",
    "Therefore the file appears in the right hand side bar.\n",
    "\n",
    "Move your cursor on the cell bellow and remove everything (cell fully empty !)\n",
    "\n",
    "Once done click the drop down arrow of the right hand side window where **HandWrittenDigit-CNN.h5** is and select **insert to code>>Insert Streaming Object**\n",
    "\n",
    "the equivalent of the following should appear with your own project COS credentials\n",
    "\n",
    "```\n",
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_8a2a8e9ef5a44a08aaca7ec89672ecaa = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='xxxxxxxxxxxxx',\n",
    "    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "# Your data file was loaded into a botocore.response.StreamingBody object.\n",
    "# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n",
    "# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n",
    "# pandas documentation: http://pandas.pydata.org/\n",
    "streaming_body_1 = client_8a2a8e9ef5a44a08aaca7ec89672ecaa.get_object(Bucket='XXXXXXX', Key='HandWrittenDigit-CNN.h5')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(streaming_body_1, \"__iter__\"): streaming_body_1.__iter__ = types.MethodType( __iter__, streaming_body_1 ) \n",
    "\n",
    "```   \n",
    "\n",
    "You also need to retrieve the bucket name of you Cloud Object Storage (COS) from the inserted code and then insert it into the cell where you will need to download files from the COS (see sample below)\n",
    "\n",
    "client_xxxxxxxx.get_object(<span style=\"background-color: #FFFF00\"> Bucket='my-generated-bucket-name-123245566788' </span>, Key='HandWrittenDigit-CNN.h5')['Body']\n",
    "\n",
    "```\n",
    "client_COS.download_file(Bucket='<inset your bucket-name here>',Key='HandWrittenDigit-CNN.h5',Filename='HandWrittenDigit-CNN.h5')\n",
    "\n",
    "```\n",
    "\n",
    "Last but not least rename the variable called **'client_8a2a8e9ef......72ecaa'** with **client_COS** (a bit more clear and reusable for the rest of the notebook !\n",
    "\n",
    "\n",
    "You're now ready to usethe HD5 definition and weights for your model instead of having to retrain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_8a2a8e9ef5a44a08aaca7ec89672ecaa = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='SgL3gHSfOX7WRMxOLrrvDiDvOl8Z0aCkeMIL9S3j-9Ge',\n",
    "    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "# Your data file was loaded into a botocore.response.StreamingBody object.\n",
    "# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n",
    "# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n",
    "# pandas documentation: http://pandas.pydata.org/\n",
    "streaming_body_1 = client_8a2a8e9ef5a44a08aaca7ec89672ecaa.get_object(Bucket='demoai-donotdelete-pr-odc7lk3sakuluh', Key='_mini_XCEPTION.102-0.66.hdf5')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(streaming_body_1, \"__iter__\"): streaming_body_1.__iter__ = types.MethodType( __iter__, streaming_body_1 ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Client_COS = client_8a2a8e9ef5a44a08aaca7ec89672ecaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model to be retrain :  0\n",
      "HandWrittenDigit-CNN.h5\n"
     ]
    }
   ],
   "source": [
    "ModelFile = 'HandWrittenDigit-CNN.h5'\n",
    "\n",
    "ReTrainModel = 5\n",
    "try:\n",
    "    # Replace the below bucket name by your own bucket project name.    \n",
    "    Client_COS.download_file(Bucket='demoai-donotdelete-pr-odc7lk3sakuluh', Key=ModelFile,Filename=ModelFile)\n",
    "except:\n",
    "    # Model never created tbd\n",
    "    RetrainModel = 1\n",
    "else:\n",
    "    RetrainModel = 0\n",
    "print(\"Model to be retrain : \", RetrainModel)\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "\n",
    "def base_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "if RetrainModel == 0:\n",
    "    cnn_n = load_model(ModelFile)\n",
    "    cnn_n.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "else:\n",
    "    cnn_n = base_model()\n",
    "\n",
    "cnn_n.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####     28   28    1\n",
      "              Conv2D    \\|/  -------------------       320     0.0%\n",
      "                relu   #####     26   26   32\n",
      "              Conv2D    \\|/  -------------------     18496     1.5%\n",
      "                relu   #####     24   24   64\n",
      "        MaxPooling2D   Y max -------------------         0     0.0%\n",
      "                       #####     12   12   64\n",
      "             Dropout    | || -------------------         0     0.0%\n",
      "                       #####     12   12   64\n",
      "             Flatten   ||||| -------------------         0     0.0%\n",
      "                       #####        9216\n",
      "               Dense   XXXXX -------------------   1179776    98.3%\n",
      "                relu   #####         128\n",
      "             Dropout    | || -------------------         0     0.0%\n",
      "                       #####         128\n",
      "               Dense   XXXXX -------------------      1290     0.1%\n",
      "             softmax   #####          10\n"
     ]
    }
   ],
   "source": [
    "# Vizualizing model structure\n",
    "sequential_model_to_ascii_printout(cnn_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "print(y_train.shape)\n",
    "if RetrainModel == 1:\n",
    "    cnn = cnn_n.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RetrainModel == 1:\n",
    "    scores = cnn_n.evaluate(x_test, y_test, verbose=0)\n",
    "    print(scores)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RetrainModel == 1:\n",
    "    cnn_n.save(ModelFile)\n",
    "    ClientCOS.upload_file(Bucket='demoai-donotdelete-pr-odc7lk3sakuluh', Key=ModelFile,Filename=ModelFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Storing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "\n",
    "wml_client = WatsonMachineLearningAPIClient(WML_CREDENTIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘mnist_cnn.tar*’: No such file or directory\n",
      "mnist_cnn.h5\n"
     ]
    }
   ],
   "source": [
    "cnn_n.save(\"mnist_cnn.h5\")\n",
    "!rm mnist_cnn.tar*\n",
    "!tar -czvf mnist_cnn.tar.gz mnist_cnn.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm mnist_cnn.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MNIST Model\"\n",
    "\n",
    "# Update the FRAMEWORK_VERSION below depending on the tensorflow version used\n",
    "model_meta = {\n",
    "    wml_client.repository.ModelMetaNames.NAME: model_name,\n",
    "    wml_client.repository.ModelMetaNames.DESCRIPTION: \"MNIST model\",\n",
    "    wml_client.repository.ModelMetaNames.FRAMEWORK_NAME: \"tensorflow\",\n",
    "    wml_client.repository.ModelMetaNames.FRAMEWORK_VERSION: \"1.11\",\n",
    "    wml_client.repository.ModelMetaNames.FRAMEWORK_LIBRARIES: [\n",
    "         {\"name\": \"keras\", \"version\": \"2.2.4\"}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------  -------------------------------  ------------------------  ---------------  -----------------\n",
      "GUID                                  NAME                             CREATED                   FRAMEWORK        TYPE\n",
      "13bc8f83-ec48-41e5-ba28-dee6154e2080  Spark German Risk Model - Final  2019-11-08T10:03:42.944Z  mllib            definition\n",
      "91e232a5-0bc8-4916-81f7-19be81aa33bf  Spark German Risk Model - Final  2019-10-02T10:13:10.364Z  mllib            definition\n",
      "812f030f-56c7-4231-a24e-0d0ed9fb917c  Spark German Risk Model - Final  2019-09-30T19:39:32.021Z  mllib            definition\n",
      "85c80a15-a03e-45b6-904d-806a291e260a  Spark German Risk Model - Final  2019-09-30T17:51:50.181Z  mllib            definition\n",
      "60d3a487-1f32-4fd4-9a49-7ea85001306d  Spark German Risk Model - Final  2019-09-30T17:50:29.777Z  mllib            definition\n",
      "35a9ed96-fc11-4129-9a96-f83f71020694  Spark German Risk Model - Final  2019-09-30T17:46:37.635Z  mllib            definition\n",
      "540264f8-7fe8-4ea4-896c-50e00175d9e9  Spark German Risk Model - Final  2019-09-30T17:45:47.523Z  mllib            definition\n",
      "83c9b392-20cd-4c63-89c2-882c250a26ff  Spark German Risk Model - Final  2019-09-30T17:43:26.744Z  mllib            definition\n",
      "43fb2e21-e4fc-4573-9686-2f674f526610  Spark German Risk Model - Final  2019-09-30T17:41:27.191Z  mllib            definition\n",
      "15382117-d170-407d-aea6-9adfc00fbce5  Spark German Risk Model - Final  2019-09-30T17:28:57.386Z  mllib            definition\n",
      "71cb4f66-68cf-4305-b17b-c4714d132d2e  Spark German Risk Model - Final  2019-09-29T11:33:31.607Z  mllib            definition\n",
      "50d44e6f-282a-45c2-b7f8-be34359b2591  Spark German Risk Model - Final  2019-09-23T14:10:19.047Z  mllib            definition\n",
      "0495091a-cacd-43e6-be23-4d80c13c987c  Text Binary Classifier           2019-09-18T16:13:34.693Z  mllib            definition\n",
      "0b87bef5-5e35-42fa-b630-3b04c1dd784a  Text Binary Classifier           2019-09-18T10:30:17.718Z  mllib            definition\n",
      "a70ff561-104a-4201-b10d-4d62464086d8  Text Binary Classifier           2019-09-18T09:09:43.694Z  mllib            definition\n",
      "e946bfbf-ea9e-46ca-9fe1-240b55ba4743  Text Binary Classifier           2019-09-17T15:04:37.361Z  mllib            definition\n",
      "21e378e8-3cd4-4d22-ba53-366bba18e3a2  FER-Model-HDF5                   2019-11-11T13:55:43.357Z  tensorflow-1.11  model\n",
      "6c48ad35-8ec4-4b0a-9390-d63d0b8442bf  FER-Model-HDF5                   2019-11-11T13:34:54.507Z  tensorflow-1.11  model\n",
      "7a50683b-66dd-4e63-b44f-5c2799dfcdd6  MNIST Model                      2019-11-10T07:51:53.660Z  tensorflow-1.11  model\n",
      "5408520e-b01f-46aa-9767-9e690bd02f3e  MNIST Model                      2019-11-08T14:56:22.452Z  tensorflow-1.11  model\n",
      "cbe4d658-aba7-4d0b-9bad-831238446281  MNIST Model                      2019-11-08T11:31:55.317Z  tensorflow-1.11  model\n",
      "32944fbd-6503-4a1d-9b0b-2cf968a6d169  MNIST Model                      2019-11-08T11:23:40.638Z  tensorflow-1.11  model\n",
      "c018330b-44b8-4a7d-9d3c-cbabb7d5e239  MNIST Model                      2019-11-08T11:13:53.134Z  tensorflow-1.11  model\n",
      "dbf094de-0d46-48f7-948c-c84f6b8b248a  MNIST Model                      2019-11-08T11:06:30.001Z  tensorflow-1.11  model\n",
      "183bcc4d-7169-4ac4-8239-4acc23f592e1  MNIST Model                      2019-11-08T10:48:37.575Z  tensorflow-1.11  model\n",
      "dcf1e624-2696-4997-aaa5-35af4fdb4aa4  Spark German Risk Model - Final  2019-11-08T10:03:50.316Z  mllib-2.3        model\n",
      "c852301e-c62c-4f7b-ab09-307cb01403f4  MNIST Model                      2019-11-07T15:52:38.664Z  tensorflow-1.11  model\n",
      "815f7ce2-632f-4b1a-88d0-7c738d000bf7  MNIST Model                      2019-11-07T15:20:43.898Z  tensorflow-1.11  model\n",
      "4f13023a-b135-4cd8-888e-d16bf1fab28c  MNIST Model                      2019-11-07T15:19:22.070Z  tensorflow-1.11  model\n",
      "3019b211-19e8-4052-b268-99934c7dacc5  MNIST Model                      2019-11-07T15:15:10.354Z  tensorflow-1.11  model\n",
      "dbb150b0-e542-4f15-b5f5-ecb669db1f42  MNIST Model                      2019-11-07T14:56:41.645Z  tensorflow-1.11  model\n",
      "fcc006a9-2801-499c-81a3-c89508715e29  MNIST Model                      2019-11-07T14:44:58.489Z  tensorflow-1.11  model\n",
      "683bc719-9691-449a-919e-e2e8b2d33f44  MNIST Model                      2019-11-07T14:35:04.243Z  tensorflow-1.11  model\n",
      "c035f6ab-11b8-48c7-9a39-c4cd41cac11e  Simpsons300                      2019-11-06T10:09:44.008Z  tensorflow-1.5   model\n",
      "8fd859c9-acfa-4bec-a1da-d32726d58cd7  FER-Model-HDF5                   2019-10-16T20:21:05.302Z  tensorflow-1.11  model\n",
      "1fb1d948-111e-41ed-af51-c9e34f661f5a  MNIST Model                      2019-10-02T14:17:54.362Z  tensorflow-1.11  model\n",
      "ee1bed7b-6eb4-44e9-a5a0-f4ff995de644  FER-Kaggle                       2019-09-26T12:40:59.874Z  tensorflow-1.5   model\n",
      "c62f61ca-67dd-41d5-8c06-faff03f6c883  MNIST Model                      2019-09-25T12:52:56.362Z  tensorflow-1.11  model\n",
      "390d32b2-b3f0-4f67-aceb-6d9b04b0db0a  Text Binary Classifier           2019-09-18T16:13:50.846Z  mllib-2.3        model\n",
      "35ff37c5-b12b-437b-b269-809cdd815e27  Text Binary Classifier           2019-09-18T09:09:49.035Z  mllib-2.3        model\n",
      "3f0f33c8-42a8-46a2-98f9-2407bbca2511  Text Binary Classifier           2019-09-17T15:04:42.888Z  mllib-2.3        model\n",
      "4b540dfa-400d-4160-a399-f72d166409a5  GermanCreditRiskModel            2019-09-16T12:51:34.237Z  mllib-2.3        model\n",
      "f4f0a9ff-1a19-4d95-bb63-df884413f52f  FER-Model-HDF5 Deployment        2019-11-11T13:55:45.679Z  tensorflow-1.11  online deployment\n",
      "448084c5-1e3d-4485-9c91-df88a2b6ab78  FER-Model-HDF5 Deployment        2019-11-11T13:34:56.918Z  tensorflow-1.11  online deployment\n",
      "8947f73d-5f5e-4353-a1cb-7a18d24223f0  MNIST Model Deployment           2019-11-10T07:51:56.304Z  tensorflow-1.11  online deployment\n",
      "53f2dddb-be2b-4ca6-a5fc-0b60a4ccd770  MNIST Model Deployment           2019-11-08T14:56:25.368Z  tensorflow-1.11  online deployment\n",
      "6541e80d-f70b-4991-8af0-46fb1314855f  MNIST Model Deployment           2019-11-08T11:31:58.204Z  tensorflow-1.11  online deployment\n",
      "91b8f685-7c35-4c49-b0f4-1970838a53c4  MNIST Model Deployment           2019-11-08T11:23:43.449Z  tensorflow-1.11  online deployment\n",
      "52f3bd15-41db-4b35-a82f-c2176803a711  MNIST Model Deployment           2019-11-08T11:13:55.702Z  tensorflow-1.11  online deployment\n",
      "df59a0b8-f09c-4422-9884-e83dca7c6006  MNIST Model Deployment           2019-11-08T11:06:32.491Z  tensorflow-1.11  online deployment\n",
      "------------------------------------  -------------------------------  ------------------------  ---------------  -----------------\n",
      "Note: Only first 50 records were displayed. To display more use more specific list functions.\n"
     ]
    }
   ],
   "source": [
    " wml_client.repository.list()\n",
    "# wml_client.repository.delete('')\n",
    "# wml_client.deployments.delete('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# published_model_details = wml_client.repository.get_details('7d7d20b6-6e54-4643-ae14-f99f41f0f986')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Model of framework tensorflow and versions 1.5/1.11 has been deprecated. These versions will not be supported after 26th Nov 2019.\n"
     ]
    }
   ],
   "source": [
    "published_model_details = wml_client.repository.store_model(model='mnist_cnn.tar.gz', meta_props=model_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0350bda8-6d1a-4763-a0a4-9c7070527ad7'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_uid = wml_client.repository.get_model_uid(published_model_details)\n",
    "model_uid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Model of framework tensorflow and versions 1.5/1.11 has been deprecated. These versions will not be supported after 26th Nov 2019.\n",
      "\n",
      "\n",
      "#######################################################################################\n",
      "\n",
      "Synchronous deployment creation for uid: '0350bda8-6d1a-4763-a0a4-9c7070527ad7' started\n",
      "\n",
      "#######################################################################################\n",
      "\n",
      "\n",
      "INITIALIZING\n",
      "DEPLOY_IN_PROGRESS..\n",
      "DEPLOY_SUCCESS\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "Successfully finished deployment creation, deployment_uid='eb5b0436-a33a-4297-92db-8a2d3126ee86'\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deployment= wml_client.deployments.create(name= model_name + \" Deployment\", model_uid=model_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://us-south.ml.cloud.ibm.com/v3/wml_instances/febb80c2-33af-4014-8dd8-ef2170ff4cfb/deployments/eb5b0436-a33a-4297-92db-8a2d3126ee86/online\n"
     ]
    }
   ],
   "source": [
    "scoring_url = wml_client.deployments.get_scoring_url(deployment)\n",
    "print(scoring_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Subscriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Configuring OpenScale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.17'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ibm_ai_openscale import APIClient\n",
    "from ibm_ai_openscale.engines import WatsonMachineLearningAsset\n",
    "\n",
    "aios_client = APIClient(AIOS_CREDENTIALS)\n",
    "aios_client.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing subscription for MNIST Model\n"
     ]
    }
   ],
   "source": [
    "#  CLEAN SUBSCRIPTION ENTRIES\n",
    "subscriptions_uids = aios_client.data_mart.subscriptions.get_uids()\n",
    "for subscription in subscriptions_uids:\n",
    "    sub_name = aios_client.data_mart.subscriptions.get_details(subscription)['entity']['asset']['name']\n",
    "    if sub_name == model_name:\n",
    "        aios_client.data_mart.subscriptions.delete(subscription)\n",
    "        print('Deleted existing subscription for', model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Subscribe the asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<HTML>\n",
       "        <body>\n",
       "            <h3>Subscriptions</h3>\n",
       "            <table style='border: 1px solid #dddddd; font-family: Courier'>\n",
       "                <th style='border: 1px solid #dddddd'>uid</th><th style='border: 1px solid #dddddd'>name</th><th style='border: 1px solid #dddddd'>type</th><th style='border: 1px solid #dddddd'>binding_uid</th><th style='border: 1px solid #dddddd'>created</th>\n",
       "                <tr><td style='border: 1px solid #dddddd'>ba8e4e44-5b90-459d-9aa8-fe04631e15e4</td><td style='border: 1px solid #dddddd'>FER-Model-HDF5</td><td style='border: 1px solid #dddddd'>model</td><td style='border: 1px solid #dddddd'>febb80c2-33af-4014-8dd8-ef2170ff4cfb</td><td style='border: 1px solid #dddddd'>2019-11-11T15:26:14.501Z</td></tr><tr><td style='border: 1px solid #dddddd'>087a04a9-2318-472e-ad42-3783b631666b</td><td style='border: 1px solid #dddddd'>Spark German Risk Model - Final</td><td style='border: 1px solid #dddddd'>model</td><td style='border: 1px solid #dddddd'>febb80c2-33af-4014-8dd8-ef2170ff4cfb</td><td style='border: 1px solid #dddddd'>2019-11-08T10:04:43.254Z</td></tr><tr><td style='border: 1px solid #dddddd'>b5079da2-264b-43e8-a71f-a9ee23208832</td><td style='border: 1px solid #dddddd'>FER-Kaggle</td><td style='border: 1px solid #dddddd'>model</td><td style='border: 1px solid #dddddd'>febb80c2-33af-4014-8dd8-ef2170ff4cfb</td><td style='border: 1px solid #dddddd'>2019-10-16T11:26:28.635Z</td></tr><tr><td style='border: 1px solid #dddddd'>c50ada6b-a76e-42be-b000-831d519dda63</td><td style='border: 1px solid #dddddd'>FER-2013</td><td style='border: 1px solid #dddddd'>model</td><td style='border: 1px solid #dddddd'>febb80c2-33af-4014-8dd8-ef2170ff4cfb</td><td style='border: 1px solid #dddddd'>2019-10-09T22:18:02.029Z</td></tr>\n",
       "            </table>\n",
       "        </body>\n",
       "        </HTML>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ibm_ai_openscale.supporting_classes import *\n",
    "\n",
    "aios_client.data_mart.subscriptions.list()\n",
    "# aios_client.data_mart.subscriptions.delete('657c48a9-d29a-4e29-a215-b8a28046bfd3')\n",
    "\n",
    "Asset = WatsonMachineLearningAsset(model_uid,\n",
    "                                   problem_type=ProblemType.MULTICLASS_CLASSIFICATION,\n",
    "                                   input_data_type=InputDataType.UNSTRUCTURED_IMAGE,\n",
    "                                   probability_column='probability'\n",
    "                                  )\n",
    "subscription = aios_client.data_mart.subscriptions.add(Asset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<HTML>\n",
       "        <body>\n",
       "            <h3>Subscriptions</h3>\n",
       "            <table style='border: 1px solid #dddddd; font-family: Courier'>\n",
       "                <th style='border: 1px solid #dddddd'>uid</th><th style='border: 1px solid #dddddd'>name</th><th style='border: 1px solid #dddddd'>type</th><th style='border: 1px solid #dddddd'>binding_uid</th><th style='border: 1px solid #dddddd'>created</th>\n",
       "                <tr><td style='border: 1px solid #dddddd'>e56ffa07-970d-4d74-b284-1e1e03244544</td><td style='border: 1px solid #dddddd'>MNIST Model</td><td style='border: 1px solid #dddddd'>model</td><td style='border: 1px solid #dddddd'>febb80c2-33af-4014-8dd8-ef2170ff4cfb</td><td style='border: 1px solid #dddddd'>2019-11-12T14:14:59.425Z</td></tr><tr><td style='border: 1px solid #dddddd'>ba8e4e44-5b90-459d-9aa8-fe04631e15e4</td><td style='border: 1px solid #dddddd'>FER-Model-HDF5</td><td style='border: 1px solid #dddddd'>model</td><td style='border: 1px solid #dddddd'>febb80c2-33af-4014-8dd8-ef2170ff4cfb</td><td style='border: 1px solid #dddddd'>2019-11-11T15:26:14.501Z</td></tr><tr><td style='border: 1px solid #dddddd'>087a04a9-2318-472e-ad42-3783b631666b</td><td style='border: 1px solid #dddddd'>Spark German Risk Model - Final</td><td style='border: 1px solid #dddddd'>model</td><td style='border: 1px solid #dddddd'>febb80c2-33af-4014-8dd8-ef2170ff4cfb</td><td style='border: 1px solid #dddddd'>2019-11-08T10:04:43.254Z</td></tr><tr><td style='border: 1px solid #dddddd'>b5079da2-264b-43e8-a71f-a9ee23208832</td><td style='border: 1px solid #dddddd'>FER-Kaggle</td><td style='border: 1px solid #dddddd'>model</td><td style='border: 1px solid #dddddd'>febb80c2-33af-4014-8dd8-ef2170ff4cfb</td><td style='border: 1px solid #dddddd'>2019-10-16T11:26:28.635Z</td></tr><tr><td style='border: 1px solid #dddddd'>c50ada6b-a76e-42be-b000-831d519dda63</td><td style='border: 1px solid #dddddd'>FER-2013</td><td style='border: 1px solid #dddddd'>model</td><td style='border: 1px solid #dddddd'>febb80c2-33af-4014-8dd8-ef2170ff4cfb</td><td style='border: 1px solid #dddddd'>2019-10-09T22:18:02.029Z</td></tr>\n",
       "            </table>\n",
       "        </body>\n",
       "        </HTML>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aios_client.data_mart.subscriptions.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity': {'asset': {'asset_id': '0350bda8-6d1a-4763-a0a4-9c7070527ad7',\n",
       "   'asset_type': 'model',\n",
       "   'created_at': '2019-11-12T14:14:22.580Z',\n",
       "   'name': 'MNIST Model',\n",
       "   'url': 'https://us-south.ml.cloud.ibm.com/v3/wml_instances/febb80c2-33af-4014-8dd8-ef2170ff4cfb/published_models/0350bda8-6d1a-4763-a0a4-9c7070527ad7'},\n",
       "  'asset_properties': {'input_data_type': 'unstructured_image',\n",
       "   'model_type': 'tensorflow-1.11',\n",
       "   'probability_fields': ['probability'],\n",
       "   'problem_type': 'multiclass',\n",
       "   'runtime_environment': 'None Provided'},\n",
       "  'configurations': [{'enabled': True,\n",
       "    'monitor_definition_id': 'payload_logging',\n",
       "    'type': 'payload_logging',\n",
       "    'url': '/v1/data_marts/70ee9046-f34e-441c-8dbe-75d57d88b6f7/service_bindings/febb80c2-33af-4014-8dd8-ef2170ff4cfb/subscriptions/e56ffa07-970d-4d74-b284-1e1e03244544/configurations/payload_logging'},\n",
       "   {'enabled': False,\n",
       "    'monitor_definition_id': 'explainability',\n",
       "    'type': 'explainability',\n",
       "    'url': '/v1/data_marts/70ee9046-f34e-441c-8dbe-75d57d88b6f7/service_bindings/febb80c2-33af-4014-8dd8-ef2170ff4cfb/subscriptions/e56ffa07-970d-4d74-b284-1e1e03244544/configurations/explainability'},\n",
       "   {'enabled': True,\n",
       "    'monitor_definition_id': 'performance_monitoring',\n",
       "    'type': 'performance_monitoring',\n",
       "    'url': '/v1/data_marts/70ee9046-f34e-441c-8dbe-75d57d88b6f7/service_bindings/febb80c2-33af-4014-8dd8-ef2170ff4cfb/subscriptions/e56ffa07-970d-4d74-b284-1e1e03244544/configurations/performance_monitoring'},\n",
       "   {'enabled': False,\n",
       "    'monitor_definition_id': 'fairness_monitoring',\n",
       "    'type': 'fairness_monitoring',\n",
       "    'url': '/v1/data_marts/70ee9046-f34e-441c-8dbe-75d57d88b6f7/service_bindings/febb80c2-33af-4014-8dd8-ef2170ff4cfb/subscriptions/e56ffa07-970d-4d74-b284-1e1e03244544/configurations/fairness_monitoring'},\n",
       "   {'enabled': False,\n",
       "    'monitor_definition_id': 'correlations',\n",
       "    'type': 'correlations',\n",
       "    'url': '/v1/data_marts/70ee9046-f34e-441c-8dbe-75d57d88b6f7/service_bindings/febb80c2-33af-4014-8dd8-ef2170ff4cfb/subscriptions/e56ffa07-970d-4d74-b284-1e1e03244544/configurations/correlations'},\n",
       "   {'enabled': False,\n",
       "    'monitor_definition_id': 'drift',\n",
       "    'type': 'drift',\n",
       "    'url': '/v1/data_marts/70ee9046-f34e-441c-8dbe-75d57d88b6f7/service_bindings/febb80c2-33af-4014-8dd8-ef2170ff4cfb/subscriptions/e56ffa07-970d-4d74-b284-1e1e03244544/configurations/drift'},\n",
       "   {'enabled': False,\n",
       "    'monitor_definition_id': 'quality',\n",
       "    'type': 'quality_monitoring',\n",
       "    'url': '/v1/data_marts/70ee9046-f34e-441c-8dbe-75d57d88b6f7/service_bindings/febb80c2-33af-4014-8dd8-ef2170ff4cfb/subscriptions/e56ffa07-970d-4d74-b284-1e1e03244544/configurations/quality'},\n",
       "   {'enabled': False,\n",
       "    'monitor_definition_id': 'my_model_performance',\n",
       "    'type': 'my_model_performance',\n",
       "    'url': '/v1/data_marts/70ee9046-f34e-441c-8dbe-75d57d88b6f7/service_bindings/febb80c2-33af-4014-8dd8-ef2170ff4cfb/subscriptions/e56ffa07-970d-4d74-b284-1e1e03244544/configurations/my_model_performance'}],\n",
       "  'deployments': [{'created_at': '2019-11-12T14:14:22.640Z',\n",
       "    'deployment_id': 'eb5b0436-a33a-4297-92db-8a2d3126ee86',\n",
       "    'deployment_rn': '',\n",
       "    'deployment_type': 'online',\n",
       "    'name': 'MNIST Model Deployment',\n",
       "    'scoring_endpoint': {'request_headers': {'Content-Type': 'application/json'},\n",
       "     'url': 'https://us-south.ml.cloud.ibm.com/v3/wml_instances/febb80c2-33af-4014-8dd8-ef2170ff4cfb/deployments/eb5b0436-a33a-4297-92db-8a2d3126ee86/online'},\n",
       "    'url': 'https://us-south.ml.cloud.ibm.com/v3/wml_instances/febb80c2-33af-4014-8dd8-ef2170ff4cfb/deployments/eb5b0436-a33a-4297-92db-8a2d3126ee86'}],\n",
       "  'service_binding_id': 'febb80c2-33af-4014-8dd8-ef2170ff4cfb',\n",
       "  'status': {'state': 'active'}},\n",
       " 'metadata': {'guid': 'e56ffa07-970d-4d74-b284-1e1e03244544',\n",
       "  'url': '/v1/data_marts/70ee9046-f34e-441c-8dbe-75d57d88b6f7/service_bindings/febb80c2-33af-4014-8dd8-ef2170ff4cfb/subscriptions/e56ffa07-970d-4d74-b284-1e1e03244544',\n",
       "  'created_at': '2019-11-12T14:14:59.425Z'}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subscription.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Score the model and get transaction-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/envs/Python36/lib/python3.6/site-packages (1.15.4)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/Python36/lib/python3.6/site-packages (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from matplotlib) (1.15.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from matplotlib) (2.7.5)\n",
      "Requirement already satisfied: six in /opt/conda/envs/Python36/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/Python36/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.1.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "img = np.array(x_test[77], dtype='float')\n",
    "pixels = img.reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "    \"fields\": [\n",
      "        \"prediction\",\n",
      "        \"prediction_classes\",\n",
      "        \"probability\"\n",
      "    ],\n",
      "    \"values\": [\n",
      "        [\n",
      "            [\n",
      "                3.840007047983818e-05,\n",
      "                3.6366909625940025e-06,\n",
      "                0.9999328851699829,\n",
      "                1.4511347501411365e-07,\n",
      "                2.573256274729374e-09,\n",
      "                3.4113309399508296e-10,\n",
      "                3.763992673100347e-09,\n",
      "                2.2245205400395207e-05,\n",
      "                2.4453431706206175e-06,\n",
      "                2.6317934498365503e-07\n",
      "            ],\n",
      "            2,\n",
      "            [\n",
      "                3.840007047983818e-05,\n",
      "                3.6366909625940025e-06,\n",
      "                0.9999328851699829,\n",
      "                1.4511347501411365e-07,\n",
      "                2.573256274729374e-09,\n",
      "                3.4113309399508296e-10,\n",
      "                3.763992673100347e-09,\n",
      "                2.2245205400395207e-05,\n",
      "                2.4453431706206175e-06,\n",
      "                2.6317934498365503e-07\n",
      "            ]\n",
      "        ]\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "Wait(20)\n",
    "\n",
    "scoring_data = {'values': [x_test[77].tolist()]}\n",
    "predictions = wml_client.deployments.score(scoring_url, scoring_data)\n",
    "print(json.dumps(predictions, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'648e088bab6e54d81303cc1744a03233-1'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wait(20)\n",
    "transaction_id = subscription.payload_logging.get_table_content().scoring_id[0]\n",
    "transaction_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"explainability\"></a>\n",
    "## 4. Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Configure Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription.explainability.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enabled': True}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subscription.explainability.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Get explanation for the transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================\n",
      "\n",
      " Looking for explanation for 648e088bab6e54d81303cc1744a03233-1 \n",
      "\n",
      "================================================================\n",
      "\n",
      "\n",
      "\n",
      "in_progress.........\n",
      "finishedSomething went wrong\n",
      "{\"trace\":\"c0de5ec42ececc4981bd7cd3a9c13a76\",\"errors\":[{\"code\":\"not_found\",\"message\":\"Requested object could not be found.\"}]}\n"
     ]
    }
   ],
   "source": [
    "explanation = ()\n",
    "try :\n",
    "    explanation = subscription.explainability.run(transaction_id, background_mode=False,cem=False)\n",
    "except:\n",
    "  print(\"Something went wrong\")\n",
    "  wml_client.repository.delete(model_uid)\n",
    "  deployment_id = wml_client.deployments.get_uid(deployment)\n",
    "  wml_client.deployments.delete(deployment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error in the preious cell\n",
    "something ending by **KeyError: 'cem_state'**\n",
    "it's a bug :( in the library, but still you can collect the transaction_id from the upper cell\n",
    "\n",
    "```\n",
    "# Wait(60)\n",
    "transaction_id = subscription.payload_logging.get_table_content().scoring_id[0]\n",
    "transaction_id\n",
    "```\n",
    "\n",
    "then open the following webpage and paste the transaction_id and search for it, you will see the result of the image model explainability.\n",
    "\n",
    "https://aiopenscale.cloud.ibm.com/aiopenscale/explain\n",
    "\n",
    "    \n",
    "**Explaining image model transactions**\n",
    "\n",
    "For an image classification model example of explainability, you can see which parts of an image contributed positively to the predicted outcome and which contributed negatively. In the following example, the image in the positive pane shows the parts which impacted positively to the prediction and the image in the negative pane shows the parts of images that had a negative impact on the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print (json.dumps(explanation,  sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The explanation images can be obtained using the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pillow\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "\n",
    "imgOrigin = explanation[\"entity\"][\"predictions\"][0][\"explanation_features\"][0][\"full_image\"]\n",
    "img_data = base64.b64decode(imgOrigin)\n",
    "OriginPic = Image.open(io.BytesIO(img_data)).resize((128, 128)).convert('RGBA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = explanation[\"entity\"][\"predictions\"][1][\"explanation_features\"][0][\"full_image\"]\n",
    "img_data = base64.b64decode(img)\n",
    "ExpPic = Image.open(io.BytesIO(img_data)).resize((128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Background = ExpPic.convert('RGBA')\n",
    "\n",
    "# \"data\" is a height x width x 4 numpy array\n",
    "data = np.array(Background)\n",
    "\n",
    "# Temporarily unpack the bands for readability\n",
    "red, green, blue, alpha = data.T \n",
    "\n",
    "# Replace white with red... (leaves alpha values alone...)\n",
    "white_areas = (red != 0) | (blue != 0) | (green != 0)\n",
    "data[..., :-1][white_areas.T] = (255, 0, 0) # Transpose back needed\n",
    "\n",
    "Background = Image.fromarray(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.blend(Background, OriginPic,alpha=0.3).resize((256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_client.repository.delete(model_uid)\n",
    "deployment_id = wml_client.deployments.get_uid(deployment)\n",
    "wml_client.deployments.delete(deployment_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
